% Local IspellDict: en
\chapter{Introduction}\label{ch01:intro}

The rapid advancement of large language models (LLMs) has fundamentally transformed the landscape of natural language processing and artificial intelligence. While models such as GPT-4, LLaMA, and Mistral demonstrate remarkable generative capabilities, they remain constrained by a critical limitation: their knowledge is static, bounded by a training cutoff, and prone to hallucination when queried beyond their parametric memory (Lewis et al., 2020; Maynez et al., 2020). This disconnect between the fluency of generated text and its factual grounding represents one of the central open problems in deploying LLMs in real-world, knowledge-intensive applications.

Retrieval-Augmented Generation (RAG) emerged as a compelling solution to this challenge, augmenting the generative process with a retrieval mechanism that fetches relevant documents from an external corpus at inference time (Lewis et al., 2020). By conditioning generation on retrieved context, RAG systems reduce hallucination, enable access to up-to-date information, and improve interpretability through grounded responses. The paradigm has since gained considerable traction across domains including open-domain question answering, enterprise knowledge management, and scientific literature synthesis.

Despite its promise, the standard RAG pipeline — typically a single-pass retrieve-then-generate architecture — exhibits well-documented weaknesses. Dense retrieval methods based on bi-encoder embeddings, while semantically expressive, suffer from domain sensitivity and fail to capture exact lexical matches effectively (Karpukhin et al., 2020). Sparse retrieval methods such as BM25, conversely, excel at term-level precision but lack semantic generalisation. Hybrid retrieval strategies that fuse both paradigms have shown empirical improvements (Ma et al., 2021), yet their integration remains inconsistent across implementations. Furthermore, the ranking of retrieved documents prior to generation — a step with outsized impact on output quality — is frequently underpowered, with bi-encoder similarity scores serving as a poor proxy for contextual relevance. Cross-encoder reranking models address this gap but introduce latency that naive implementations fail to manage efficiently (Nogueira  Cho, 2019).

Beyond retrieval quality, single-turn RAG architectures are fundamentally ill-suited for complex, multi-step queries that require iterative reasoning, evidence synthesis across multiple documents, or dynamic adjustment of retrieval strategy based on intermediate findings. These limitations have motivated a shift towards agentic RAG, wherein the retrieval and generation process is governed by an autonomous reasoning agent capable of planning, self-evaluation, tool use, and replanning (Yao et al., 2023; Shinn et al., 2023). Agentic frameworks such as ReAct, Reflexion, and AutoGPT demonstrate that LLMs can act as decision-making agents, but their application to RAG remains nascent, with few unified frameworks offering modularity, efficiency, and reproducibility simultaneously.

A further practical barrier concerns computational accessibility. The dominant paradigm for deploying capable LLMs relies on cloud-based inference, raising concerns around data privacy, latency, cost, and reproducibility. The emergence of efficient model compression techniques — including quantization and hardware-specific inference optimisation — has opened a pathway for high-quality local inference. In particular, Apple's MLX framework enables performant LLM inference and fine-tuning on Apple Silicon, yet its integration into research-grade RAG systems has received limited attention.

This thesis addresses these converging gaps by proposing an efficient and modular framework for advanced agentic RAG. The framework combines hybrid retrieval (FAISS-based dense retrieval and BM25-based sparse retrieval) with cross-encoder reranking, and extends the pipeline with agentic capabilities including multi-step reasoning, self-evaluation, and dynamic replanning — all designed to run efficiently on local hardware via MLX. The work is motivated by three principal research questions:

1. How can dense and sparse retrieval be effectively fused within a unified, modular pipeline to maximise retrieval precision and recall across heterogeneous queries?
2. To what extent do agentic mechanisms — specifically iterative reasoning, self-evaluation, and dynamic replanning — improve response quality over standard single-pass RAG in complex, multi-hop question answering scenarios?
3. How can such a framework be implemented efficiently for local inference on consumer hardware, without sacrificing performance relative to cloud-dependent baselines?

By addressing these questions, this thesis contributes a reproducible, extensible framework that advances the state of the art in agentic RAG, while simultaneously lowering the barrier to deployment through efficient local inference. The remainder of this document is structured as follows: Chapter 2 reviews the related literature in depth; Chapter 3 details the system architecture and design decisions; Chapter 4 describes the experimental methodology; Chapter 5 presents and analyses results; and Chapter 6 concludes with discussion and future directions.